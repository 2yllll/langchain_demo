{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 链 [LangChain Chain](https://python.langchain.com/en/latest/modules/chains/getting_started.html)\n",
    "\n",
    "+ 需要将多个 LLM 模型串起来；一个模型的输出，是下一个模型的输入。\n",
    "+ 需要将 LangChain 的组件，以 链式结构串起来。\n",
    "    - 读取用户输入，填入提示词模板，并调用 LLM，得到结果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(client=\"api\", model=\"text-davinci-003\", temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"商品 {product} 有什么好的名字吗？\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后用 LLM Chain 将上面两个组件串在一起："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1、水润宝丝洗衣粉 \n",
      "2、极致护色洗衣粉 \n",
      "3、芳飘活力洗衣粉 \n",
      "4、致洁亮白洗衣粉 \n",
      "5、雪纯洗衣粉 \n",
      "6、洁净无污洗衣粉 \n",
      "7、绿色洁净洗衣粉 \n",
      "8、洁净安心洗衣粉 \n",
      "9、精细慎洁洗衣粉 \n",
      "10、清新活力洗衣粉\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# .\n",
    "print(chain.run(\"京东品牌的洗衣粉\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有多个输入变量需要填，那么用 词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "京洗衣粉™\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"company\", \"product\"],\n",
    "    template=\"What is a good name for {company} that makes {product}?\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run({\n",
    "    'company': \"京东\",\n",
    "    'product': \"洗衣粉\"\n",
    "    }))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换成 Chat-模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rainbow Socks Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a company that makes {product}?\",\n",
    "            input_variables=[\"product\"],\n",
    "        )\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "chat = ChatOpenAI(client=\"api\", model=\"gpt-3.5-turbo\", temperature=0.9)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "\n",
    "print(chain.run(\"colorful socks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 调用链的方式：\n",
    "\n",
    "+ chain()\n",
    "    - 默认返回 所有的 key-value 的 词典\n",
    "    - 配置参数 `return_only_outputs` 让其只输出 必要信息；chain(\"corny\", return_only_outputs=True)\n",
    "+ chain.run()\n",
    "    - 如果 llm_chain.output_keys 只含一个值，那么 可以调用 run() 拿到最后答案\n",
    "    - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m chat \u001b[39m=\u001b[39m ChatOpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m prompt_template \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTell me a \u001b[39m\u001b[39m{adjective}\u001b[39;00m\u001b[39m joke\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m llm_chain \u001b[39m=\u001b[39m LLMChain(\n\u001b[0;32m      8\u001b[0m     llm\u001b[39m=\u001b[39mchat,\n\u001b[0;32m      9\u001b[0m     prompt\u001b[39m=\u001b[39mPromptTemplate\u001b[39m.\u001b[39mfrom_template(prompt_template)\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m llm_chain(inputs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39madjective\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mcorny\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LLMChain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "llm_chain(inputs={\"adjective\":\"corny\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
