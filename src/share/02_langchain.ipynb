{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Lang Chain](https://python.langchain.com/en/latest/use_cases/code/code-analysis-deeplake.html)\n",
    "\n",
    "用 `Lang chain` / `Deep Lake` / `GPT` 来分析 `LangChain` 库的源码。\n",
    "\n",
    "**注：** [Deep Lake](https://www.deeplake.ai/) 是个 向量数据库 (`Vector Database`)，类似于 [PineCone](https://www.pinecone.io/)\n",
    "\n",
    "流程如下：\n",
    "\n",
    "+ 准备数据\n",
    "    - 用 `TextLoader` 上传 Github LangChain库 所有的源码文件，我们把这些文件叫 `document`\n",
    "    - 用 `CharacterTextSplitter` 分割 `document` 成 块(`chunk`)\n",
    "    - 用 `OpenAIEmbedding` 将 `chunk` 转换成 嵌入向量(`Embedding Vector`)，并将向量保存到 `Deep Lake`\n",
    "+ QA 设计\n",
    "    - 用 `ChatOpenAI` 和 `ConversationalRetrievalChain` 构建 一个 `Chain` \n",
    "    - 问 问题\n",
    "    - 得到 答案"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装\n",
    "\n",
    "+ 请安装 `Python`；并在 `VsCode` 上 安装 `Python`插件，`Jupyter` 插件\n",
    "+ 请准备好 HTTP-代理，并保证 VSCode 能访问该代理；\n",
    "+ 请准备好 `OpenAI` 的 `API Key`，将其配到你个人电脑的环境变量 `OPENAI_API_KEY` 上；\n",
    "\n",
    "安装 三个 python 库： `langchain`, `deeplake`, `openai`\n",
    "\n",
    "``` bash\n",
    "pip3 install --upgrade langchain\n",
    "\n",
    "pip3 install --upgrade openai\n",
    "\n",
    "pip3 install --upgrade deeplake\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade langchain deeplake openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. 申请 Deep Lake\n",
    "\n",
    "Deep Lake 是个 向量数据库\n",
    "\n",
    "+ 到 [这里](https://app.activeloop.ai/) 注册 账号，将你的账号名修改下面的 `DEEPLAKE_ACCOUNT_NAME`\n",
    "+ 申请 api-key，并 将 api-key 填到 系统环境变量 `ACTIVELOOP_TOKEN`\n",
    "+ 运行下面的命令行：\n",
    "\n",
    "``` bash\n",
    "activeloop login -t 你申请到的ACTIVELOOP_TOKEN\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：这里要用你的 申请账号\n",
    "DEEPLAKE_ACCOUNT_NAME = \"XXX\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里要确保`LangChain`的版本是 0.0.188 或以上；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.188'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "\n",
    "# 保证langchain 用的 是 最新的 0.0.188 版本；\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 加载文件\n",
    "\n",
    "加载文件：\n",
    "\n",
    "实现先将 langchain github 库 git clone 到 本地：\n",
    "\n",
    "``` bash\n",
    "git clone https://github.com/hwchase17/langchain.git\n",
    "```\n",
    "\n",
    "没有安装 git 的童鞋，可以 点击 [这里](https://codeload.github.com/hwchase17/langchain/zip/refs/heads/master) 下载 zip 并解压\n",
    "\n",
    "用 Lang Chain 的 工具类 `TextLoader` 将文件 变 document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载 Python 文件 1565 个\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"\"\"Configuration file for the Sphinx documentation builder.\"\"\"\\n# Configuration file for the Sphinx documentation builder.\\n#\\n# This file only contains a selection of the most common options. For a full\\n# list see the documentation:\\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\\n\\n# -- Path setup --------------------------------------------------------------\\n\\n# If extensions (or modules to document with autodoc) are in another directory,\\n# add these directories to sys.path here. If the directory is relative to the\\n# documentation root, use os.path.abspath to make it absolute, like shown here.\\n#\\n# import os\\n# import sys\\n# sys.path.insert(0, os.path.abspath(\\'.\\'))\\n\\nimport toml\\n\\nwith open(\"../pyproject.toml\") as f:\\n    data = toml.load(f)\\n\\n# -- Project information -----------------------------------------------------\\n\\nproject = \"🦜🔗 LangChain\"\\ncopyright = \"2023, Harrison Chase\"\\nauthor = \"Harrison Chase\"\\n\\nversion = data[\"tool\"][\"poetry\"][\"version\"]\\nrelease = version\\n\\nhtml_title = project + \" \" + version\\nhtml_last_updated_fmt = \"%b %d, %Y\"\\n\\n\\n# -- General configuration ---------------------------------------------------\\n\\n# Add any Sphinx extension module names here, as strings. They can be\\n# extensions coming with Sphinx (named \\'sphinx.ext.*\\') or your custom\\n# ones.\\nextensions = [\\n    \"sphinx.ext.autodoc\",\\n    \"sphinx.ext.autodoc.typehints\",\\n    \"sphinx.ext.autosummary\",\\n    \"sphinx.ext.napoleon\",\\n    \"sphinx.ext.viewcode\",\\n    \"sphinxcontrib.autodoc_pydantic\",\\n    \"myst_nb\",\\n    \"sphinx_copybutton\",\\n    \"sphinx_panels\",\\n    \"IPython.sphinxext.ipython_console_highlighting\",\\n]\\nsource_suffix = [\".ipynb\", \".html\", \".md\", \".rst\"]\\n\\nautodoc_pydantic_model_show_json = False\\nautodoc_pydantic_field_list_validators = False\\nautodoc_pydantic_config_members = False\\nautodoc_pydantic_model_show_config_summary = False\\nautodoc_pydantic_model_show_validator_members = False\\nautodoc_pydantic_model_show_field_summary = False\\nautodoc_pydantic_model_members = False\\nautodoc_pydantic_model_undoc_members = False\\n# autodoc_typehints = \"signature\"\\n# autodoc_typehints = \"description\"\\n\\n# Add any paths that contain templates here, relative to this directory.\\ntemplates_path = [\"_templates\"]\\n\\n# List of patterns, relative to source directory, that match files and\\n# directories to ignore when looking for source files.\\n# This pattern also affects html_static_path and html_extra_path.\\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\\n\\n\\n# -- Options for HTML output -------------------------------------------------\\n\\n# The theme to use for HTML and HTML Help pages.  See the documentation for\\n# a list of builtin themes.\\n#\\nhtml_theme = \"sphinx_book_theme\"\\n\\nhtml_theme_options = {\\n    \"path_to_docs\": \"docs\",\\n    \"repository_url\": \"https://github.com/hwchase17/langchain\",\\n    \"use_repository_button\": True,\\n}\\n\\nhtml_context = {\\n    \"display_github\": True,  # Integrate GitHub\\n    \"github_user\": \"hwchase17\",  # Username\\n    \"github_repo\": \"langchain\",  # Repo name\\n    \"github_version\": \"master\",  # Version\\n    \"conf_py_path\": \"/docs/\",  # Path in the checkout to the docs root\\n}\\n\\n# Add any paths that contain custom static files (such as style sheets) here,\\n# relative to this directory. They are copied after the builtin static files,\\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\\nhtml_static_path = [\"_static\"]\\n\\n# These paths are either relative to html_static_path\\n# or fully qualified paths (eg. https://...)\\nhtml_css_files = [\\n    \"css/custom.css\",\\n]\\n\\nhtml_js_files = [\\n    \"js/mendablesearch.js\",\\n]\\n\\nnb_execution_mode = \"off\"\\nmyst_enable_extensions = [\"colon_fence\"]', metadata={'source': './langchain/docs\\\\conf.py'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# LangChain库的本地 clone 目录\n",
    "root_dir = './langchain/'\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    for file in filenames:\n",
    "        # 仅处理 Python 文件\n",
    "        if file.endswith('.py') and '/.venv/' not in dirpath:\n",
    "            try: \n",
    "                loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "                \n",
    "                document_list = loader.load_and_split()\n",
    "\n",
    "                # 将 document_list flat 到 docs 去；\n",
    "                docs.extend(document_list)\n",
    "            except Exception as e: \n",
    "                pass\n",
    "\n",
    "print(f'已加载 Python 文件 {len(docs)} 个')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 分割文件\n",
    "\n",
    "分割文件：这个Demo用到文本的方式分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1006, which is longer than the specified 1000\n",
      "Created a chunk of size 1213, which is longer than the specified 1000\n",
      "Created a chunk of size 1782, which is longer than the specified 1000\n",
      "Created a chunk of size 1620, which is longer than the specified 1000\n",
      "Created a chunk of size 1485, which is longer than the specified 1000\n",
      "Created a chunk of size 1919, which is longer than the specified 1000\n",
      "Created a chunk of size 3515, which is longer than the specified 1000\n",
      "Created a chunk of size 1852, which is longer than the specified 1000\n",
      "Created a chunk of size 1533, which is longer than the specified 1000\n",
      "Created a chunk of size 1331, which is longer than the specified 1000\n",
      "Created a chunk of size 2549, which is longer than the specified 1000\n",
      "Created a chunk of size 1696, which is longer than the specified 1000\n",
      "Created a chunk of size 1086, which is longer than the specified 1000\n",
      "Created a chunk of size 1647, which is longer than the specified 1000\n",
      "Created a chunk of size 1296, which is longer than the specified 1000\n",
      "Created a chunk of size 2005, which is longer than the specified 1000\n",
      "Created a chunk of size 1235, which is longer than the specified 1000\n",
      "Created a chunk of size 1221, which is longer than the specified 1000\n",
      "Created a chunk of size 1390, which is longer than the specified 1000\n",
      "Created a chunk of size 1300, which is longer than the specified 1000\n",
      "Created a chunk of size 1185, which is longer than the specified 1000\n",
      "Created a chunk of size 1453, which is longer than the specified 1000\n",
      "Created a chunk of size 1423, which is longer than the specified 1000\n",
      "Created a chunk of size 1036, which is longer than the specified 1000\n",
      "Created a chunk of size 1283, which is longer than the specified 1000\n",
      "Created a chunk of size 1045, which is longer than the specified 1000\n",
      "Created a chunk of size 1501, which is longer than the specified 1000\n",
      "Created a chunk of size 1208, which is longer than the specified 1000\n",
      "Created a chunk of size 1137, which is longer than the specified 1000\n",
      "Created a chunk of size 1176, which is longer than the specified 1000\n",
      "Created a chunk of size 1176, which is longer than the specified 1000\n",
      "Created a chunk of size 1219, which is longer than the specified 1000\n",
      "Created a chunk of size 1364, which is longer than the specified 1000\n",
      "Created a chunk of size 1212, which is longer than the specified 1000\n",
      "Created a chunk of size 1950, which is longer than the specified 1000\n",
      "Created a chunk of size 1414, which is longer than the specified 1000\n",
      "Created a chunk of size 1289, which is longer than the specified 1000\n",
      "Created a chunk of size 1029, which is longer than the specified 1000\n",
      "Created a chunk of size 1269, which is longer than the specified 1000\n",
      "Created a chunk of size 3173, which is longer than the specified 1000\n",
      "Created a chunk of size 1160, which is longer than the specified 1000\n",
      "Created a chunk of size 1058, which is longer than the specified 1000\n",
      "Created a chunk of size 1260, which is longer than the specified 1000\n",
      "Created a chunk of size 1195, which is longer than the specified 1000\n",
      "Created a chunk of size 2147, which is longer than the specified 1000\n",
      "Created a chunk of size 1183, which is longer than the specified 1000\n",
      "Created a chunk of size 1253, which is longer than the specified 1000\n",
      "Created a chunk of size 1411, which is longer than the specified 1000\n",
      "Created a chunk of size 1941, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1069, which is longer than the specified 1000\n",
      "Created a chunk of size 1848, which is longer than the specified 1000\n",
      "Created a chunk of size 1418, which is longer than the specified 1000\n",
      "Created a chunk of size 2573, which is longer than the specified 1000\n",
      "Created a chunk of size 1026, which is longer than the specified 1000\n",
      "Created a chunk of size 1030, which is longer than the specified 1000\n",
      "Created a chunk of size 1230, which is longer than the specified 1000\n",
      "Created a chunk of size 1226, which is longer than the specified 1000\n",
      "Created a chunk of size 1289, which is longer than the specified 1000\n",
      "Created a chunk of size 1224, which is longer than the specified 1000\n",
      "Created a chunk of size 1031, which is longer than the specified 1000\n",
      "Created a chunk of size 1999, which is longer than the specified 1000\n",
      "Created a chunk of size 1029, which is longer than the specified 1000\n",
      "Created a chunk of size 1077, which is longer than the specified 1000\n",
      "Created a chunk of size 1120, which is longer than the specified 1000\n",
      "Created a chunk of size 1033, which is longer than the specified 1000\n",
      "Created a chunk of size 1143, which is longer than the specified 1000\n",
      "Created a chunk of size 1416, which is longer than the specified 1000\n",
      "Created a chunk of size 2482, which is longer than the specified 1000\n",
      "Created a chunk of size 1019, which is longer than the specified 1000\n",
      "Created a chunk of size 1512, which is longer than the specified 1000\n",
      "Created a chunk of size 1262, which is longer than the specified 1000\n",
      "Created a chunk of size 1092, which is longer than the specified 1000\n",
      "Created a chunk of size 2175, which is longer than the specified 1000\n",
      "Created a chunk of size 1147, which is longer than the specified 1000\n",
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 1131, which is longer than the specified 1000\n",
      "Created a chunk of size 1227, which is longer than the specified 1000\n",
      "Created a chunk of size 1233, which is longer than the specified 1000\n",
      "Created a chunk of size 1250, which is longer than the specified 1000\n",
      "Created a chunk of size 1197, which is longer than the specified 1000\n",
      "Created a chunk of size 1168, which is longer than the specified 1000\n",
      "Created a chunk of size 1352, which is longer than the specified 1000\n",
      "Created a chunk of size 1098, which is longer than the specified 1000\n",
      "Created a chunk of size 1030, which is longer than the specified 1000\n",
      "Created a chunk of size 1504, which is longer than the specified 1000\n",
      "Created a chunk of size 1038, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 1276, which is longer than the specified 1000\n",
      "Created a chunk of size 1300, which is longer than the specified 1000\n",
      "Created a chunk of size 1232, which is longer than the specified 1000\n",
      "Created a chunk of size 2108, which is longer than the specified 1000\n",
      "Created a chunk of size 1285, which is longer than the specified 1000\n",
      "Created a chunk of size 1221, which is longer than the specified 1000\n",
      "Created a chunk of size 1456, which is longer than the specified 1000\n",
      "Created a chunk of size 1181, which is longer than the specified 1000\n",
      "Created a chunk of size 1153, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1473, which is longer than the specified 1000\n",
      "Created a chunk of size 1058, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1236, which is longer than the specified 1000\n",
      "Created a chunk of size 1068, which is longer than the specified 1000\n",
      "Created a chunk of size 1318, which is longer than the specified 1000\n",
      "Created a chunk of size 1501, which is longer than the specified 1000\n",
      "Created a chunk of size 1197, which is longer than the specified 1000\n",
      "Created a chunk of size 1532, which is longer than the specified 1000\n",
      "Created a chunk of size 1535, which is longer than the specified 1000\n",
      "Created a chunk of size 1304, which is longer than the specified 1000\n",
      "Created a chunk of size 2248, which is longer than the specified 1000\n",
      "Created a chunk of size 1087, which is longer than the specified 1000\n",
      "Created a chunk of size 1096, which is longer than the specified 1000\n",
      "Created a chunk of size 2817, which is longer than the specified 1000\n",
      "Created a chunk of size 1589, which is longer than the specified 1000\n",
      "Created a chunk of size 1104, which is longer than the specified 1000\n",
      "Created a chunk of size 1449, which is longer than the specified 1000\n",
      "Created a chunk of size 1979, which is longer than the specified 1000\n",
      "Created a chunk of size 1681, which is longer than the specified 1000\n",
      "Created a chunk of size 1172, which is longer than the specified 1000\n",
      "Created a chunk of size 1237, which is longer than the specified 1000\n",
      "Created a chunk of size 1339, which is longer than the specified 1000\n",
      "Created a chunk of size 1228, which is longer than the specified 1000\n",
      "Created a chunk of size 1245, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 2082, which is longer than the specified 1000\n",
      "Created a chunk of size 1224, which is longer than the specified 1000\n",
      "Created a chunk of size 2461, which is longer than the specified 1000\n",
      "Created a chunk of size 1583, which is longer than the specified 1000\n",
      "Created a chunk of size 1048, which is longer than the specified 1000\n",
      "Created a chunk of size 1052, which is longer than the specified 1000\n",
      "Created a chunk of size 1205, which is longer than the specified 1000\n",
      "Created a chunk of size 1104, which is longer than the specified 1000\n",
      "Created a chunk of size 1010, which is longer than the specified 1000\n",
      "Created a chunk of size 1247, which is longer than the specified 1000\n",
      "Created a chunk of size 1336, which is longer than the specified 1000\n",
      "Created a chunk of size 2622, which is longer than the specified 1000\n",
      "Created a chunk of size 1040, which is longer than the specified 1000\n",
      "Created a chunk of size 1497, which is longer than the specified 1000\n",
      "Created a chunk of size 1195, which is longer than the specified 1000\n",
      "Created a chunk of size 1056, which is longer than the specified 1000\n",
      "Created a chunk of size 1626, which is longer than the specified 1000\n",
      "Created a chunk of size 1148, which is longer than the specified 1000\n",
      "Created a chunk of size 1172, which is longer than the specified 1000\n",
      "Created a chunk of size 1623, which is longer than the specified 1000\n",
      "Created a chunk of size 1367, which is longer than the specified 1000\n",
      "Created a chunk of size 1353, which is longer than the specified 1000\n",
      "Created a chunk of size 1099, which is longer than the specified 1000\n",
      "Created a chunk of size 1473, which is longer than the specified 1000\n",
      "Created a chunk of size 1303, which is longer than the specified 1000\n",
      "Created a chunk of size 1017, which is longer than the specified 1000\n",
      "Created a chunk of size 1304, which is longer than the specified 1000\n",
      "Created a chunk of size 1362, which is longer than the specified 1000\n",
      "Created a chunk of size 1380, which is longer than the specified 1000\n",
      "Created a chunk of size 1021, which is longer than the specified 1000\n",
      "Created a chunk of size 1490, which is longer than the specified 1000\n",
      "Created a chunk of size 1506, which is longer than the specified 1000\n",
      "Created a chunk of size 1137, which is longer than the specified 1000\n",
      "Created a chunk of size 1134, which is longer than the specified 1000\n",
      "Created a chunk of size 1011, which is longer than the specified 1000\n",
      "Created a chunk of size 1039, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n",
      "Created a chunk of size 1249, which is longer than the specified 1000\n",
      "Created a chunk of size 1982, which is longer than the specified 1000\n",
      "Created a chunk of size 1542, which is longer than the specified 1000\n",
      "Created a chunk of size 2432, which is longer than the specified 1000\n",
      "Created a chunk of size 1289, which is longer than the specified 1000\n",
      "Created a chunk of size 1464, which is longer than the specified 1000\n",
      "Created a chunk of size 1310, which is longer than the specified 1000\n",
      "Created a chunk of size 1152, which is longer than the specified 1000\n",
      "Created a chunk of size 1012, which is longer than the specified 1000\n",
      "Created a chunk of size 1221, which is longer than the specified 1000\n",
      "Created a chunk of size 1176, which is longer than the specified 1000\n",
      "Created a chunk of size 1410, which is longer than the specified 1000\n",
      "Created a chunk of size 1335, which is longer than the specified 1000\n",
      "Created a chunk of size 1008, which is longer than the specified 1000\n",
      "Created a chunk of size 1111, which is longer than the specified 1000\n",
      "Created a chunk of size 1010, which is longer than the specified 1000\n",
      "Created a chunk of size 2101, which is longer than the specified 1000\n",
      "Created a chunk of size 1028, which is longer than the specified 1000\n",
      "Created a chunk of size 2321, which is longer than the specified 1000\n",
      "Created a chunk of size 2826, which is longer than the specified 1000\n",
      "Created a chunk of size 1186, which is longer than the specified 1000\n",
      "Created a chunk of size 1232, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1592, which is longer than the specified 1000\n",
      "Created a chunk of size 1375, which is longer than the specified 1000\n",
      "Created a chunk of size 3466, which is longer than the specified 1000\n",
      "Created a chunk of size 1208, which is longer than the specified 1000\n",
      "Created a chunk of size 1083, which is longer than the specified 1000\n",
      "Created a chunk of size 1074, which is longer than the specified 1000\n",
      "Created a chunk of size 1411, which is longer than the specified 1000\n",
      "Created a chunk of size 2300, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割成 4765 个 chunk\n"
     ]
    }
   ],
   "source": [
    "print(f\"分割成 {len(texts)} 个 chunk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 进阶：以代码方式分割\n",
    "\n",
    "但是 可以使用 [代码方式分割](https://python.langchain.com/en/latest/modules/indexes/text_splitters/examples/code_splitter.html)，等后面一步可以继续实验；\n",
    "\n",
    "+ 注：Lang Chain 0.0.188 以上才有 Language 类\n",
    "+ 注：代码方式目前不支持 C#，不知道为什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注：langchain 0.0.188 以上才有 Language 类\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cpp',\n",
       " 'go',\n",
       " 'java',\n",
       " 'js',\n",
       " 'php',\n",
       " 'proto',\n",
       " 'python',\n",
       " 'rst',\n",
       " 'ruby',\n",
       " 'rust',\n",
       " 'scala',\n",
       " 'swift',\n",
       " 'markdown',\n",
       " 'latex',\n",
       " 'html']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 目前支持的 语言\n",
    "[e.value for e in Language]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RecursiveCharacterTextSplitter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# 可以看到 python 语言 的 分割 符号\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m RecursiveCharacterTextSplitter\u001b[39m.\u001b[39mget_separators_for_language(Language\u001b[39m.\u001b[39mPYTHON)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RecursiveCharacterTextSplitter' is not defined"
     ]
    }
   ],
   "source": [
    "# 可以看到 python 语言 的 分割 符号\n",
    "\n",
    "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='def hello_world():\\n    print(\"Hello, World!\")', metadata={}),\n",
       " Document(page_content='# Call the function\\nhello_world()', metadata={})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PYTHON_CODE = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Call the function\n",
    "hello_world()\n",
    "\"\"\"\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=50, chunk_overlap=0\n",
    ")\n",
    "\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "\n",
    "python_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 嵌入向量 并上传到 Deep Leak\n",
    "\n",
    "+ 运行的话，需要等几分钟\n",
    "+ 请在OpenAI的API项里面绑定好充值的信用卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=None, openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(client=\"davinci\")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n",
      "The dataset is private so make sure you are logged in!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/myy412001799/langchain-code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://myy412001799/langchain-code loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ingest: 100%|██████████| 5/5 [03:14<00:00\n",
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://myy412001799/langchain-code', tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape       dtype  compression\n",
      "  -------   -------    -------     -------  ------- \n",
      " embedding  generic  (4765, 1536)  float32   None   \n",
      "    ids      text     (4765, 1)      str     None   \n",
      " metadata    json     (4765, 1)      str     None   \n",
      "   text      text     (4765, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.deeplake.DeepLake at 0x1dc168e9390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake\n",
    "\n",
    "db = DeepLake.from_documents(texts, embeddings, dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\")\n",
    "\n",
    "db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. QA 回答问题\n",
    "\n",
    "+ 加载数据集\n",
    "+ 构造检索 retriever\n",
    "+ 构造 检索链 Conversational Chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载 DeepLeak 数据库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/myy412001799/langchain-code\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://myy412001799/langchain-code loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://myy412001799/langchain-code already exists, loading from the storage\n",
      "Dataset(path='hub://myy412001799/langchain-code', read_only=True, tensors=['embedding', 'ids', 'metadata', 'text'])\n",
      "\n",
      "  tensor     htype      shape       dtype  compression\n",
      "  -------   -------    -------     -------  ------- \n",
      " embedding  generic  (4765, 1536)  float32   None   \n",
      "    ids      text     (4765, 1)      str     None   \n",
      " metadata    json     (4765, 1)      str     None   \n",
      "   text      text     (4765, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " \r"
     ]
    }
   ],
   "source": [
    "# 加载数据库\n",
    "\n",
    "db = DeepLake(\n",
    "    dataset_path=f\"hub://{DEEPLAKE_ACCOUNT_NAME}/langchain-code\", \n",
    "    read_only=True, \n",
    "    embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造 检索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['fetch_k'] = 20\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以利用过滤器检索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(x):\n",
    "    # filter based on source code\n",
    "    if 'something' in x['text'].data()['value']:\n",
    "        return False\n",
    "    \n",
    "    # filter based on path e.g. extension\n",
    "    metadata =  x['metadata'].data()['value']\n",
    "    return 'only_this' in metadata['source'] or 'also_that' in metadata['source']\n",
    "\n",
    "### turn on below for custom filtering\n",
    "# retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造检索链："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "model = ChatOpenAI(client=\"chatgpt\", model='gpt-3.5-turbo') # 'ada' 'gpt-3.5-turbo' 'gpt-4',\n",
    "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问答开始："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: 哪些类从类Chain中派生? \n",
      "\n",
      "**Answer**: 从类Chain中派生的类有：\n",
      "- APIChain\n",
      "- AnalyzeDocumentChain\n",
      "- BaseCombineDocumentsChain\n",
      "- BaseQAWithSourcesChain\n",
      "- ChatVectorDBChain\n",
      "- ConstitutionalChain\n",
      "- ConversationalRetrievalChain\n",
      "- ConversationChain\n",
      "- FlareChain\n",
      "- GraphCypherQAChain\n",
      "- GraphQAChain\n",
      "- HypotheticalDocumentEmbedder\n",
      "- LLMChain\n",
      "- LLMBashChain\n",
      "- LLMCheckerChain\n",
      "- LLMMathChain\n",
      "- LLMRequestsChain\n",
      "- LLMSummarizationCheckerChain\n",
      "- MapReduceChain\n",
      "- OpenAIModerationChain\n",
      "- PALChain\n",
      "- QAGenerationChain\n",
      "- QAWithSourcesChain\n",
      "- RetrievalQA\n",
      "- RetrievalQAWithSourcesChain\n",
      "- RouterChain\n",
      "- SequentialChain\n",
      "- SimpleSequentialChain\n",
      "- SQLDatabaseChain\n",
      "- SQLDatabaseSequentialChain\n",
      "- TransformChain\n",
      "- VectorDBQA\n",
      "- VectorDBQAWithSourcesChain \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"哪些类从类Chain中派生?\",\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "] \n",
    "chat_history = []\n",
    "\n",
    "for question in questions:  \n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result['answer']))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
